{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Workflow\n",
    "\n",
    "1. Load and explore the data\n",
    "2. Preprocess the data\n",
    "3. Extract features\n",
    "4. Train the model\n",
    "5. Evaluate the model\n",
    "6. Analyze model behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from typing import TypeAlias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = \"/Users/johnhuang/Desktop/coding/Bag_of_Words/Data\"\n",
    "FILEPATH = f\"{FOLDER}/Tweets_5K.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath: str) -> tuple[list[str], list[int]]:\n",
    "    \"\"\"\n",
    "    Loads Twitter data into two lists.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    raw_tweets : list[str]\n",
    "        A list of all Tweets in the dataset\n",
    "    labels : list[int]\n",
    "        A list of the sentiments corresponding to each raw tweet encoded as integers,\n",
    "        -1 meaning negative, 0 meaning neutral, and 1 meaning positive\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(filepath)\n",
    "    raw_tweets = dataset[\"text\"].astype(str).tolist()\n",
    "    labels=[]\n",
    "    for label in dataset[\"sentiment\"].astype(str).tolist():\n",
    "      if label == \"neutral\":\n",
    "        labels.append(0)\n",
    "      elif label == \"negative\":\n",
    "        labels.append(-1)\n",
    "      else:\n",
    "        labels.append(1)\n",
    "    return (raw_tweets, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tweets, labels = load_data(FILEPATH)\n",
    "for p, label in zip(raw_tweets[:10], labels[:10]):\n",
    "    print(f\"{label}:\\t{p}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Plot Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "pd.Series(labels).value_counts().plot.bar(title=\"Sentiment Distribution in Tweets\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Initial Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_X: list[str]) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Performs splitting on whitespace on all raw strings in a list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_X : list[str]\n",
    "        A list of raw strings (tweets)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[list[str]]\n",
    "        A list of preprocessed tweets (which are now lists of words)\n",
    "    \"\"\"\n",
    "    preprocessed_tweets = []\n",
    "    for string in raw_X:\n",
    "      preprocessed_tweets.append(string.split())\n",
    "    return preprocessed_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Featurization and Train and Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class BOW_Classifier:\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    ----------\n",
    "    clf : LogisticRegression\n",
    "        A logistic regression classifier\n",
    "    dv : DictVectorizer\n",
    "        A dictionary vectorizer for turning dictionaries into matrices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.clf = LogisticRegression(max_iter=150)\n",
    "        self.dv = DictVectorizer()\n",
    "\n",
    "    def featurize(self, preproc_X: np.ndarray[list[str]], is_test: bool = False) -> csr_matrix:\n",
    "        \"\"\"\n",
    "        Turns a list of preprocessed tweets into a binary bag-of-words matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        preproc_X : np.ndarray[list[str]]\n",
    "            A list of preprocessed tweets\n",
    "        is_test: bool, default=False\n",
    "            Whether featurization should be done using features learned during training (is_test=True)\n",
    "            or whether it should be done with features extracted from scratch using preproc_X (is_test=False)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        csr_matrix\n",
    "            A matrix with rows corresponding to tweets and columns corresponding to words\n",
    "        \"\"\"\n",
    "        vocab = []\n",
    "        for string in preproc_X:\n",
    "          tweet_dict = {}\n",
    "          for word in string:\n",
    "            tweet_dict[word] = 1\n",
    "          vocab.append(tweet_dict)\n",
    "        if is_test:\n",
    "          matrix = self.dv.transform(vocab)\n",
    "        else:\n",
    "          matrix = self.dv.fit_transform(vocab)\n",
    "        return matrix\n",
    "\n",
    "    def train(self, X_train: np.ndarray[list[str]], y_train: np.ndarray[int]):\n",
    "        \"\"\"\n",
    "        Trains the BOW classifier on the given training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : np.ndarray[list[str]]\n",
    "            Preprocessed tweets for training\n",
    "        y_train : np.ndarray[int]\n",
    "            Sentiments corresponding to the tweets in X_train\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        BoW_matrix = self.featurize(X_train, is_test = False)\n",
    "        self.clf.fit(BoW_matrix, y_train)\n",
    "\n",
    "    def test(self, X_test: np.ndarray[list[str]]) -> np.ndarray[int]:\n",
    "        \"\"\"\n",
    "        Classifies the given test data and returns predicted sentiments.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_test : np.ndarray[list[str]]\n",
    "            Preprocessed tweets for testing\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : np.ndarray[int]\n",
    "            Predicted sentiments for the tweets in X_test\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        BoW_test_matrix = self.featurize(X_test, is_test = True)\n",
    "        return self.clf.predict(BoW_test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def run_kfold_crossval(\n",
    "    model: BOW_Classifier, X: list[list[str]], y: list[int], k: int = 5\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Executes k-fold cross-validation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : BOW_Classifier\n",
    "        A BOW model that has train and test methods\n",
    "    X : list[list[str]]\n",
    "        Preprocessed tweets for training and testing\n",
    "    y : list[int]\n",
    "        Sentiments corresponding to the tweets in X\n",
    "    k : int, default=5\n",
    "        The number of folds to use for cross-validation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[float]\n",
    "        A list of accuracy values from testing with each fold\n",
    "    \"\"\"\n",
    "    accuracy_values = [0] * k\n",
    "    X, y = np.array(X, dtype=list), np.array(y, dtype=int)\n",
    "    skf = StratifiedKFold(n_splits = k)\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X,y)):\n",
    "      training_tweets, training_sentiments = X[train_index], y[train_index]\n",
    "      model.train(training_tweets, training_sentiments)\n",
    "      pred = model.test(X[test_index])\n",
    "      accuracy = accuracy_score(y[test_index], pred)\n",
    "      accuracy_values[i] = accuracy\n",
    "    return accuracy_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perfs(perfs: list[list[float]], names: list[str], k: int = 5):\n",
    "    \"\"\"\n",
    "    Plots performances of models in a bar chart.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    perfs : list[list[float]]\n",
    "        A list of accuracy results for each model\n",
    "    names : list[str]\n",
    "        The names of each of the models (in the same order as perfs)\n",
    "    k : int, default=5\n",
    "        The value of k used for cross-validation when producing the performances\n",
    "    \"\"\"\n",
    "    means = []\n",
    "    stds = []\n",
    "    for i, perf in enumerate(perfs):\n",
    "        mean = np.mean(perf)\n",
    "        means.append(mean)\n",
    "        stds.append(np.std(perf))\n",
    "        print(\"%s:\\t%.03f\" % (names[i], mean))\n",
    "    plt.bar(np.arange(len(means)), means, yerr=stds)\n",
    "    plt.xticks(np.arange(len(names)), names)\n",
    "    plt.ylabel(f\"Accuracy with {k} Folds\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "K_FOLD = 10\n",
    "raw_tweets, y = load_data(FILEPATH)\n",
    "\n",
    "X_preproc = preprocess(raw_tweets)\n",
    "bow_model = BOW_Classifier()\n",
    "basic_bow_accs = run_kfold_crossval(bow_model, X_preproc, y, k=K_FOLD)\n",
    "\n",
    "# here, we are going generate the \"most frequent class\" baseline based on the training data\n",
    "counts = Counter(y).values()\n",
    "mfc_baseline = [max(counts) / sum(counts)] * K_FOLD\n",
    "\n",
    "# plot the results!\n",
    "plot_perfs([mfc_baseline, basic_bow_accs], [\"MFC Baseline\", \"Basic BOW\"], k=K_FOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_doc: TypeAlias = spacy.tokens.doc.Doc\n",
    "\n",
    "NUM_TWEETS = 5000  # INFO: Feel free to change this to load in fewer tweets when debugging, but otherwise keep it at 5000\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "CACHE_PATH = f\"{FOLDER}/parsed_{NUM_TWEETS}_tweets.pickle\"\n",
    "\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    print(f\"Loading parsed tweets from cache at {CACHE_PATH}\")\n",
    "    parsed_tweets = pickle.load(open(CACHE_PATH, \"rb\"))\n",
    "else:\n",
    "    # parse all the tweets with spacy\n",
    "    parsed_tweets = []\n",
    "    for i, r in enumerate(raw_tweets):\n",
    "        if i == NUM_TWEETS:\n",
    "            break\n",
    "        parsed_tweets.append(nlp(r))\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"Processed {i + 1} out of {len(raw_tweets)}\", end=\"\\r\")\n",
    "    print(\"Processing complete\")\n",
    "    if CACHE_PATH is not None:\n",
    "        pickle.dump(parsed_tweets, open(CACHE_PATH, \"wb\"))\n",
    "\n",
    "print(f\"{len(parsed_tweets)} parsed tweets loaded.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
